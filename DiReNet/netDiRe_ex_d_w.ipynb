{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "torch.cuda.set_device(0)\n",
    "import scipy.io as sio\n",
    "from thop import profile\n",
    "import matplotlib.pyplot as plt\n",
    "from network.utils import *\n",
    "from network.direnet4 import *\n",
    "# def setup_seed(seed):\n",
    "#      torch.manual_seed(seed)\n",
    "#      torch.cuda.manual_seed_all(seed)\n",
    "#      np.random.seed(seed)\n",
    "#      random.seed(seed)\n",
    "#      torch.backends.cudnn.deterministic = True\n",
    "# setup_seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ex = 5\n",
    "d_ex = 4\n",
    "model_name = 'DiReNetD5_d{}'.format(d_ex)\n",
    "reduction = 16\n",
    "R = 3\n",
    "lamda = 0\n",
    "flag = -1\n",
    "epochs = 1000\n",
    "batch_size = 200 \n",
    "lr=3e-4\n",
    "lr_mi=1e-4\n",
    "if flag == -1:\n",
    "    nums=0\n",
    "else:\n",
    "    nums=1\n",
    "z_dim = 2048//reduction//R\n",
    "w_dim = 2048//reduction - 2*z_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat = sio.loadmat('./dataset/CDLC300.mat')\n",
    "dataP = mat['dataP']\n",
    "dataP = torch.from_numpy(dataP)\n",
    "dataP = (dataP-dataP.min())/(dataP.max()-dataP.min())\n",
    "train_dataloader = DataLoader(dataP[0:100000,:], batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(dataP[100000:120000,:], batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "class net(nn.Module):\n",
    "    def __init__(self,reduction,R):\n",
    "        super().__init__()\n",
    "        self.en = DiReNetE()\n",
    "        self.cs = DiReNetC32(reduction,R)\n",
    "        self.de = eval(model_name)()\n",
    "    def forward(self, data):\n",
    "        xy,x,y = self.en(data)\n",
    "        zw,zx,zy,x_c,y_c = self.cs(xy,x,y)\n",
    "        x_hat,y_hat = self.de(x_c,y_c)\n",
    "        return xy,zw,zx,zy,x_hat,y_hat\n",
    "model = net(reduction,R).cuda()\n",
    "if flag == -1:\n",
    "    mi_estimator = CLUBSample(1, 1, 1).cuda() # nothing\n",
    "if flag == 0:\n",
    "    mi_estimator = CLUBSample(z_dim, z_dim, z_dim).cuda() # zx and zy\n",
    "if flag == 1:\n",
    "    mi_estimator = CLUBSample(2048, w_dim, 2048).cuda() # data and zw\n",
    "if flag == 2:\n",
    "    mi_estimator = CLUBSample(2048, 512, 2048).cuda()# data and w\n",
    "mi_xy_estimator = CLUBSample(512, 512, 512).cuda()# datax and datay\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr )\n",
    "mi_optimizer = optim.Adam(mi_estimator.parameters(), lr = lr_mi) \n",
    "mi_xy_optimizer = optim.Adam(mi_xy_estimator.parameters(), lr = lr_mi) \n",
    "MSE_loss = nn.MSELoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_epoch = -1\n",
    "best_loss = 100\n",
    "best_nmse = 0\n",
    "best_model = model\n",
    "train_epochs_loss = []\n",
    "val_epochs_loss = []\n",
    "NMSEs = []\n",
    "mi_values = []\n",
    "mi_xy_values = []\n",
    "mi_all = []\n",
    "mi_xy_all = []\n",
    "loss_xy_mi = 0\n",
    "\n",
    "print('net:{}, reduction:{}, R:{}, ID:{}, lamda={}, flag={}'.format(model_name,reduction,R,1-2/R,lamda,flag))\n",
    "for epoch in range(epochs):\n",
    "############  train  ###############\n",
    "    mi_value = []\n",
    "    mi_xy_value = []\n",
    "    train_epoch_loss = []\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        mi_estimator.eval()\n",
    "        data = data.cuda()\n",
    "        xy,zw,zx,zy,x_hat,y_hat = model(data) \n",
    "        loss = 0.5*(MSE_loss(x_hat, data[:,:, 0:16,:]) + MSE_loss(y_hat, data[:,:, 16:32,:]))\n",
    "\n",
    "        if flag == -1:\n",
    "            loss_mi = 0 # nothing\n",
    "        if flag == 0:\n",
    "            loss_mi = mi_estimator(zx, zy) # zx and zy\n",
    "        if flag == 1:\n",
    "            loss_mi = mi_estimator(data.view(batch_size, -1), zw) # data and zw\n",
    "        if flag == 2:\n",
    "            loss_mi = mi_estimator(data.view(batch_size, -1), xy.view(batch_size, -1))# data and w     \n",
    "        if flag!=-1 and loss_mi.item()<0:\n",
    "            loss_mi = loss_mi-loss_mi         \n",
    "        LOSS = loss + lamda * (loss_mi-loss_xy_mi)**2  \n",
    "        optimizer.zero_grad()\n",
    "        LOSS.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        for j in range(nums):\n",
    "            mi_xy_optimizer.train()\n",
    "            mi_loss = mi_xy_optimizer.learning_loss(data[:,:, 0:16,:].view(batch_size, -1),  data[:,:, 16:32,:].view(batch_size, -1))\n",
    "            mi_all.append(mi_xy_optimizer(data[:,:, 0:16,:].view(batch_size, -1),  data[:,:, 16:32,:].view(batch_size, -1)).item())\n",
    "            mi_xy_optimizer.zero_grad()\n",
    "            mi_loss.backward()\n",
    "            mi_xy_optimizer.step()\n",
    "        loss_xy_mi = mi_all.mean()\n",
    "        for k in range(nums):\n",
    "            model.eval()\n",
    "            mi_estimator.train()\n",
    "            xy,zw,zx,zy,x_hat,y_hat = model(data)\n",
    "            if flag == -1:\n",
    "                mi_loss = mi_estimator.learning_loss(zx, zy)\n",
    "                mi_all.append(mi_estimator(zx, zy).item())# nothing\n",
    "            if flag == 0:\n",
    "                mi_loss = mi_estimator.learning_loss(zx, zy)\n",
    "                mi_all.append(mi_estimator(zx, zy).item())# zx and zy\n",
    "            if flag == 1:\n",
    "                mi_loss = mi_estimator.learning_loss(data.view(batch_size, -1), zw)\n",
    "                mi_all.append(mi_estimator(data.view(batch_size, -1), zw).item()) # data and zw\n",
    "            if flag == 2:\n",
    "                mi_loss = mi_estimator.learning_loss(data.view(batch_size, -1), xy.view(batch_size, -1))\n",
    "                mi_all.append(mi_estimator(data.view(batch_size, -1), xy.view(batch_size, -1)).item())# data and w\n",
    "            mi_optimizer.zero_grad()\n",
    "            mi_loss.backward()\n",
    "            mi_optimizer.step()\n",
    "        if flag !=-1:    \n",
    "            mi_value.append(loss_mi.item())\n",
    "        mi_xy_value.append(loss_xy_mi)\n",
    "        train_epoch_loss.append(loss.item())\n",
    "    train_epochs_loss.append(np.average(train_epoch_loss))\n",
    "    mi_values.append(np.average(mi_value))\n",
    "    mi_xy_values.append(np.average(mi_xy_value))\n",
    "############  val  ###############\n",
    "    model.eval()\n",
    "    val_epoch_loss = []\n",
    "    NMSE = []\n",
    "    for idx, data in enumerate(val_dataloader): \n",
    "        data = data.cuda()\n",
    "        xy,zw,zx,zy,x_hat,y_hat = model(data) \n",
    "        loss = 0.5*(MSE_loss(x_hat, data[:,:, 0:16,:]) + MSE_loss(y_hat, data[:,:, 16:32,:]))\n",
    "        sparse_gt = data - 0.5\n",
    "        sparse_pred = torch.cat((x_hat,y_hat),dim=2) - 0.5\n",
    "        power_gt = sparse_gt[:, 0, :, :] ** 2 + sparse_gt[:, 1, :, :] ** 2\n",
    "        difference = sparse_gt - sparse_pred\n",
    "        mse_gt = difference[:, 0, :, :] ** 2 + difference[:, 1, :, :] ** 2\n",
    "        lossDB = (mse_gt.sum(dim=[1, 2]) / power_gt.sum(dim=[1, 2])).mean()\n",
    "        val_epoch_loss.append(loss.item())\n",
    "        NMSE.append(10*np.log10(lossDB.item()))\n",
    "    val_epochs_loss.append(np.average(val_epoch_loss))\n",
    "    NMSEs.append(np.average(NMSE))\n",
    "############  save best  ###############\n",
    "    if val_epochs_loss[epoch] < best_loss:\n",
    "        best_epoch = epoch\n",
    "        best_loss = val_epochs_loss[epoch]\n",
    "        best_nmse = NMSEs[epoch]\n",
    "        np.savetxt(\"./seg_txt/EX_D{}W{}r{}.txt\".format(d_ex,w_ex,reduction),[best_epoch,best_nmse],fmt='%.4f')\n",
    "        best_model = model\n",
    "        torch.save(best_model.state_dict(), \"./seg_model/EX_D{}W{}r{}.pth\".format(d_ex,w_ex,reduction))\n",
    "############  print  ###############\n",
    "    if epoch%10 == 0:\n",
    "        print(\"epoch={}/{}, lr={:.3e}, train_loss={:.3e}, val_loss={:.3e}, nmse={}, mi_xy_z={:.3e}, mi_x_y={:.3e}\".format(epoch, epochs,\\\n",
    "            optimizer.state_dict()['param_groups'][0]['lr'],train_epochs_loss[epoch],val_epochs_loss[epoch],NMSEs[epoch],mi_values[epoch],mi_xy_values[epoch]))\n",
    "    if epoch%50==0:\n",
    "        print(\"best_epoch={},best_loss={:.3e},best_nmse={}\".format(best_epoch,best_loss,best_nmse))\n",
    "        logging.info(\"best_epoch={},best_loss={:.3e},best_nmse={}\".format(best_epoch,best_loss,best_nmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('net:{}, reduction:{}, R:{}, ID:{}, lamda={}, flag={}'.format(model_name,reduction,R,1-2/R,lamda,flag))\n",
    "print('best_epoch={}, best_val_loss={}, best_nmse={}'.format(best_epoch,best_loss,best_nmse))\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(train_epochs_loss[1:], label='train')\n",
    "plt.plot(val_epochs_loss[1:], label='val')\n",
    "plt.legend()\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(mi_all,label='MI_xy_z')\n",
    "plt.plot(mi_xy_all,label='MI_x_y')\n",
    "plt.legend()\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(NMSEs, label='NMSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "final = np.array(NMSEs)\n",
    "np.save(\"./seg_nmse/EX_D{}W{}r{}.npy\".format(d_ex,w_ex,reduction),final) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
